{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Psychological condition analysis and classification using SVD\n",
        "\n",
        "Aim: To develop a system that analyzes social media posts to predict psychological conditions such as social anxiety disorder, depression, anxiety, suicidal tendencies, mania, psychosis, substance use disorder, and others using Singular Value Decomposition (SVD)\n",
        "\n",
        "Dataset: https://www.kaggle.com/datasets/suchintikasarkar/sentiment-analysis-for-mental-health\n",
        "\n",
        "Reference Paper - https://www.engr.uvic.ca/~seng474/svd.pdf\n"
      ],
      "metadata": {
        "id": "QEdETqJiuc0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Workflow Overview:\n",
        "\n",
        "1. Data Acquisition and Loading:\n",
        "Load the dataset from Kaggle into a pandas DataFrame for analysis.\n",
        "\n",
        "2. Data Preprocessing:<br/>\n",
        "a. Text Cleaning: Remove special characters, numbers, and URLs.<br/>\n",
        "b. Tokenization: Split text into individual words.<br/>\n",
        "c. Stop Words Removal: Remove common words that do not contribute to the sentiment.<br/>\n",
        "d. Stemming/Lemmatization: Reduce words to their base or root form by removing suffixes or other techniques.<br/>\n",
        "Lemmatization provides a greater accuracy compared to stemming, at the cost of computation.<br/>\n",
        "\n",
        "3. Creation of TF-IDF Vector:\n",
        "Convert the cleaned text into numerical features using Term Frequency-Inverse Document Frequency (TF-IDF) vectorization. This allows us to create a sparse matrix, which represents all the posts/textual values in the dataset\n",
        "\n",
        "4. Dimensionality Reduction using SVD:\n",
        "Since the sparse matrix created using TFIDF Vectorizer would be too huge, it would be computationally expensive to use the entire matrix for any psychological pattern classification task.\n",
        "Hence, we apply SVD to the TF-IDF matrix to reduce dimensionality and capture the most significant features.\n",
        "\n",
        "5. Model Building and Training:\n",
        "Split the data into training and testing sets.\n",
        "Train a machine learning classifier (e.g., Logistic Regression, Support Vector Machine) using the reduced features.\n",
        "\n",
        "6. Model Evaluation:\n",
        "Evaluate the model's performance using metrics such as accuracy, precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "VJrEEQDLw6dn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "LfVyYVBwZdAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "id": "114O-lN5I-Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "sO5X-d55u874"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C31RCHwuWaB"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"suchintikasarkar/sentiment-analysis-for-mental-health\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1"
      ],
      "metadata": {
        "id": "OA8I5qVbuyli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading the dataset and evaluating the same\n",
        "df = pd.read_csv('/root/.cache/kagglehub/datasets/suchintikasarkar/sentiment-analysis-for-mental-health/versions/1/Combined Data.csv')"
      ],
      "metadata": {
        "id": "5MZhN5pnuzpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "mTWaJUc4vEyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping unnecessary values\n",
        "df = df.drop(columns = ['Unnamed: 0'])"
      ],
      "metadata": {
        "id": "WdKNKqicvF3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "HIbLNQCnvF8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "3eaZs5ZdvRUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping Null values\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "eT0-OgvlvVti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "8Zv3ftcGvYqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping duplicated values\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "dKdWAZ1-vYt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(inplace = True)"
      ],
      "metadata": {
        "id": "xAGujb6fvvfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "WPQ_6b7uvYxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "6-5-bsKfwi6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing 1st row since it is erroneous\n",
        "df = df.iloc[1:,:]"
      ],
      "metadata": {
        "id": "cPdrxAFdvF_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading stopwords\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "T6FxhJKWyyOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization: Used this instead of Stemming since we intend to have greater accuracy\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # Remove special characters and numbers\n",
        "    text = re.sub(r'\\\\',' ', text) #Remove backslash characters\n",
        "    text = text.lower()\n",
        "    tokens = text.split()\n",
        "\n",
        "    #Removing stop words as well as applying lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing to the 'text' column\n",
        "df['proc_stmnt'] = df['statement'].apply(preprocess_text)\n",
        "df['proc_stmnt']"
      ],
      "metadata": {
        "id": "W28GVvO11B7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a copy of the dataset to be used later\n",
        "df_og = df.copy()"
      ],
      "metadata": {
        "id": "wez-kb7LT73l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conducting Classification"
      ],
      "metadata": {
        "id": "tIIpRdjg1LIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Identifying unique status for the posts in the dataset\n",
        "df.status.value_counts()"
      ],
      "metadata": {
        "id": "QMrG_HRLTz9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.countplot(data = df, x = 'status', hue = 'status',palette = 'rocket')\n",
        "plt.xticks(rotation = 45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_dEnd5G-c-JQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.loc[df['status'].isin(['Normal', 'Depression','Bipolar','Personality disorder'])]\n",
        "#df1 = df.copy()"
      ],
      "metadata": {
        "id": "A0lLg0uMRvyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize TF-IDF Vectorizer and create a full sparse matrix with all the features (inidicated by max_features having None value)\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=None)\n",
        "x_tfidf = tfidf_vectorizer.fit_transform(df1['proc_stmnt'])"
      ],
      "metadata": {
        "id": "SBwSJ4q6Rr4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using SVD\n",
        "#NLP tasks using LSA (Latent Semantic Analysis) or SVD often use 100–500 components for best performance.\n",
        "#Google’s research on LSA for NLP suggests 300–500 components work well.\n",
        "svd = TruncatedSVD(n_components=500)\n",
        "\n",
        "# Fit and transform the TF-IDF features\n",
        "x_svd = svd.fit_transform(x_tfidf)"
      ],
      "metadata": {
        "id": "9k8QgxEpRr4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting into training and testing\n",
        "# Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df1['status'])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_svd, y_encoded, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "ca4J3XP65Yzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Identifying the encoding rule used\n",
        "label_encoder.inverse_transform([0, 1, 2, 3])"
      ],
      "metadata": {
        "id": "xcR3a_UK9jv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the models to be used\n",
        "from xgboost import XGBClassifier\n",
        "classification_models = {\n",
        "    'Logistic Regressor': LogisticRegression(),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state = 42),\n",
        "    'Random Forest Classifier': RandomForestClassifier(n_estimators = 47, random_state = 47),\n",
        "    'AdaBoost': AdaBoostClassifier(random_state = 13),\n",
        "    'Ridge Classifier':RidgeClassifier(),\n",
        "    'Extra Trees Classifier':ExtraTreesClassifier(n_estimators = 68, random_state = 71),\n",
        "    'XGBoost Classifier': XGBClassifier(\n",
        "    objective='multi:softmax',  # For multiclass classification\n",
        "    num_class=len(label_encoder.classes_),  # Number of unique classes\n",
        "    eval_metric='mlogloss',  # Multiclass log loss\n",
        "    random_state=42)\n",
        "}"
      ],
      "metadata": {
        "id": "ef9cJrSEbWFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = None\n",
        "best_score = 0\n",
        "\n",
        "print('\\nClassification Models: ')\n",
        "for name, model in classification_models.items():\n",
        "  pipeline = Pipeline([\n",
        "      ('classifier',model)\n",
        "  ])\n",
        "\n",
        "  #Training the model\n",
        "  pipeline.fit(X_train, y_train)\n",
        "\n",
        "  #Prediction\n",
        "  ypred = pipeline.predict(X_test)\n",
        "\n",
        "  #Evaluation\n",
        "  accuracy = accuracy_score(y_test, ypred)\n",
        "  #f1 = f1_score(y_test, ypred)\n",
        "  print(f'Accuracy for {name}: {accuracy:.2f}')\n",
        "  print(f'Classification Report:')\n",
        "  print(classification_report(y_test, ypred))\n",
        "  #print('F1 Score: ', f1)\n",
        "\n",
        "  if best_score < accuracy:\n",
        "    best_score = accuracy\n",
        "    best_model = name"
      ],
      "metadata": {
        "id": "pMymjwUXcOS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The best classification model is {best_model} with an accuracy of {best_score:.4f}.\")"
      ],
      "metadata": {
        "id": "MFrA7F0jdRUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "### Final Thoughts\n",
        "We can make a real time sentiment analysis system to check a users posts to analyze for any underlying psychological conditions. To do the same, we would need to:\n",
        "1. Freeze the best model that we have found (XGB Classifier)\n",
        "2. Create a pipeline that preprocesses new posts and converts them into TFIDF vectors followed by SVD.\n",
        "3. Finally, the model will predict the condition of the user based on the preprocessed input.\n",
        "4. The output will be filtered so that any person with psychological conditions will be reached out to, for support and any assistance needed."
      ],
      "metadata": {
        "id": "LmTKNCFMhYoM"
      }
    }
  ]
}